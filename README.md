# Open Source LLMs

| **Model**          | **Parameters**       | **License**               | **Key Features**                                                                 | **Source** |  
|---------------------|----------------------|---------------------------|---------------------------------------------------------------------------------|------------|  
| **Llama 3**         | 1B–405B             | Llama Community License   | Scalable, multilingual, multimodal, supports 8k–128k context windows.          | Meta       |  
| **Mistral**         | 3B–124B             | Apache 2.0                | High performance for edge/on-device AI, native function calling, MoE support.  | Mistral AI |  
| **Falcon 3**        | 1B–10B              | TII Falcon License        | Efficient for resource-constrained environments, multilingual support.         | TII         |  
| **Gemma 2**         | 2B–27B              | Gemma License             | Focus on responsible AI, optimized for inference.                              | Google     |  
| **Phi-3.x/4**       | 3.8B–42B (MoE)      | Microsoft Research License| Math reasoning, multilingual, image understanding, on-device inference.        | Microsoft  |  
| **StarCoder2**      | 3B–15B              | Apache 2.0                | Code generation, multi-language programming.                                   | BigCode    |  
| **Yi**              | 6B–34B              | Apache 2.0                | Bilingual (English/Chinese), math/code tasks, 200k context window.             | 01.AI      |  
| **Qwen2.5**         | 0.5B–72B            | Qwen License / Apache 2.0 | Structured data processing, multilingual, 128k context.                        | Alibaba    |  
| **Bloom**           | 176B                | OpenRAIL-M v1             | Multilingual (46 languages), strong for translation tasks.                     | Hugging Face |  
| **OPT**             | 125M–175B           | Apache 2.0                | Zero-shot capabilities, flexible deployment.                                   | Meta       |  
| **Dolly**           | 3B–12B              | MIT                       | Instruction-tuned, ideal for chatbots and task-oriented dialogue.              | Databricks |  
| **StableLM-Alpha**  | 3B–65B              | CC BY-SA-4.0              | Long-context (4k tokens), document summarization.                              | Stability AI |  
| **OpenChatKit**     | 1.3B                | MIT                       | Ethical alignment, reduced toxicity, privacy-focused.                          | Together Computer |  
| **OLMo**            | 7B                  | Apache 2.0                | Transparency-focused, reproducible training code.                              | Allen Institute for AI |  
| **RWKV**            | Up to 14B           | Apache 2.0                | RNN-based, infinite context length, efficient inference.                       | RWKV Foundation |  
